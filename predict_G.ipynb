{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config, models\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main variables\n",
    "dataset_name               = \"WN11\"\n",
    "embedding_model            = models.TransE\n",
    "model_timestamp            = '1524623630'\n",
    "max_knn_k                  = 7\n",
    "knn_k_start                = 3 # start at # nearest neighbors\n",
    "knn_k_step                 = 2 # predict with 5 more neighbors for each generator\n",
    "\n",
    "# GPU settings\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './benchmarks/' + dataset_name\n",
    "import_path = './results/{}/{}/{}/'.format(\n",
    "    dataset_name,\n",
    "    embedding_model.__name__,\n",
    "    model_timestamp\n",
    ")\n",
    "g_hat_path = import_path + '/g_hat/'\n",
    "log_info = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_df = pd.read_csv('{}/model_info.tsv'.format(import_path), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform model info into dict with only one \"row\"\n",
    "model_info = model_info_df.to_dict()\n",
    "for key,d in model_info.iteritems():\n",
    "    model_info[key] = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()\n",
    "dataset_path = \"./benchmarks/{}/\".format(model_info['dataset_name'])\n",
    "con.set_in_path(dataset_path)\n",
    "con.set_test_link_prediction(False)\n",
    "con.set_test_triple_classification(True)\n",
    "con.set_work_threads(multiprocessing.cpu_count())\n",
    "con.set_dimension(int(model_info['k']))\n",
    "con.score_norm = model_info['score_norm']\n",
    "con.init()\n",
    "con.set_model(embedding_model)\n",
    "con.import_variables(\"{}tf_model/model.vec.tf\".format(import_path)) # loading model via tensor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export embedding parameters (save to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.save_parameters(import_path + '/embedding.vec.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embedding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = con.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['ent_embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(dataset_path + 'train2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'rel'])\n",
    "valid = pd.read_csv(dataset_path + 'valid2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'rel'])\n",
    "test = pd.read_csv(dataset_path + 'test2id.txt', sep=' ', skiprows=1, names=['head', 'tail', 'rel'])\n",
    "\n",
    "valid_neg = pd.read_csv(dataset_path + 'valid2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'rel'])\n",
    "test_neg = pd.read_csv(dataset_path + 'test2id_neg.txt', sep=' ', skiprows=1, names=['head', 'tail', 'rel'])\n",
    "\n",
    "pos_train_and_valid = pd.concat([train, valid])\n",
    "data = pd.concat([train, valid, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import dataset_tools\n",
    "tc_dict = dataset_tools.read_type_constrain_file(dataset_path + '/type_constrain.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all neighbors (Train KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=max_knn_k, n_jobs=8).fit(params['ent_embeddings'])\n",
    "knn_distance, knn_indices = nbrs.kneighbors(params['ent_embeddings'])\n",
    "\n",
    "knn_learning_time = time.time() - start_time\n",
    "\n",
    "print(\"KNN learning time: {}\".format(knn_learning_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (perturbing only head or only tail) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knn_perturb_one_entity_at_a_time_generator(knn_indices, pos_train_and_valid):    \n",
    "#     for idx,row in pos_train_and_valid.iterrows():\n",
    "#         for triple in itertools.product([row['head']], knn_indices[row['tail']], [row['rel']]):\n",
    "#             yield {\n",
    "#                 'head': triple[0],\n",
    "#                 'tail': triple[1],\n",
    "#                 'rel': triple[2]\n",
    "#             }\n",
    "#         for triple in itertools.product(knn_indices[row['head']], [row['tail']], [row['rel']]):\n",
    "#             yield {\n",
    "#                 'head': triple[0],\n",
    "#                 'tail': triple[1],\n",
    "#                 'rel': triple[2]\n",
    "#             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (perturbing head, tail and products for each positive example) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_products_generator(k, knn_indices, pos_train_and_valid):    \n",
    "    for idx,row in pos_train_and_valid.iterrows():\n",
    "        for triple in itertools.product(knn_indices[row['head']][:k], knn_indices[row['tail']][:k], [row['rel']]):\n",
    "            yield {\n",
    "                'head': triple[0],\n",
    "                'tail': triple[1],\n",
    "                'rel': triple[2]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN (cartesian product head_extended X tail_extended) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knn_cartesian_product_generator(knn_indices, tc_dict):    \n",
    "#     # extend the set of all heads/tails for each relation with the k nearest neighbors\n",
    "#     extended_types = {}\n",
    "#     for rel,dic in tc_dict.iteritems():\n",
    "#         head_set = set(dic['head'])\n",
    "#         tail_set = set(dic['tail'])\n",
    "#         for ent_head,ent_tail in zip(dic['head'],dic['tail']):\n",
    "#             head_set.update(knn_indices[ent_head])\n",
    "#             tail_set.update(knn_indices[ent_tail])\n",
    "#         extended_types[rel] = {\n",
    "#             'head': head_set,\n",
    "#             'tail': tail_set\n",
    "#         }\n",
    "        \n",
    "#     for rel,dic in extended_types.iteritems():\n",
    "#         for e1e2 in itertools.product(dic['head'], dic['tail']):\n",
    "#             yield {\n",
    "#                 'head': e1e2[0],\n",
    "#                 'tail': e1e2[1],\n",
    "#                 'rel': rel\n",
    "#             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Possible Triples Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def all_possible_triples_generator(ents, rels):\n",
    "#     \"\"\"A generator for all possible triples in the current dataset (graph).\n",
    "#     Don't forget to Pray for God to make it tractable.\n",
    "    \n",
    "#     Arguments:\n",
    "#     - ents: the set of entities\n",
    "#     - rels: the set of relations\n",
    "#     \"\"\"\n",
    "#     for rel in rels:\n",
    "#         ents_perm = itertools.permutations(ents, 2)\n",
    "#         for e1e2 in ents_perm:\n",
    "#             yield {\n",
    "#                 'head': e1e2[0],\n",
    "#                 'tail': e1e2[1],\n",
    "#                 'rel': rel\n",
    "#             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions for predicting Ĝ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_from_generator(triples_iter, batch_size):\n",
    "    batch_heads = []\n",
    "    batch_tails = []\n",
    "    batch_rels = []\n",
    "    break_ = False\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            triple = next(triples_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        batch_heads.append(triple['head'])\n",
    "        batch_tails.append(triple['tail'])\n",
    "        batch_rels.append(triple['rel'])\n",
    "        \n",
    "    return (batch_heads, batch_tails, batch_rels), len(batch_heads)\n",
    "\n",
    "\n",
    "def filter_positives(heads, tails, rels, preds):\n",
    "    positive_triples = []\n",
    "    for idx_n,pred in np.ndenumerate(preds):\n",
    "        idx = idx_n[0] # ndenumerate works for the dimensional case\n",
    "        if pred == 1:\n",
    "            positive_triples.append({\n",
    "                'head': heads[idx],\n",
    "                'tail': tails[idx],\n",
    "                'relation': rels[idx]\n",
    "            })\n",
    "    return positive_triples\n",
    "\n",
    "\n",
    "def predict_g_hat(triples_iterator, batch_size=10000):\n",
    "    positive_triples = []\n",
    "    triples_count = 0\n",
    "    while True:\n",
    "        (heads, tails, rels), current_batch_size = get_batch_from_generator(triples_iterator, batch_size)\n",
    "        preds = con.classify(heads, tails, rels, batch_size)\n",
    "        positive_triples += filter_positives(heads, tails, rels, preds)\n",
    "        triples_count += current_batch_size\n",
    "        if current_batch_size < batch_size: # we are at the end of generator\n",
    "            break\n",
    "    return positive_triples, triples_count\n",
    "\n",
    "\n",
    "# def get_size_of_generator(gen):\n",
    "#     size = 0\n",
    "#     for i in gen:\n",
    "#         size += 1\n",
    "#     return size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(k, gen, batch_size=100000):\n",
    "    prediction_info = {}\n",
    "    prediction_info['knn_time'] = knn_learning_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    pos_triples, pred_size = predict_g_hat(\n",
    "        triples_iterator=gen,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    prediction_info['pred_time'] = time.time() - start_time\n",
    "    prediction_info['positive_size'] = len(pos_triples)\n",
    "    prediction_info['total_time'] = prediction_info['knn_time'] + prediction_info['pred_time']\n",
    "    prediction_info['predicted_size'] = pred_size\n",
    "    prediction_info['k'] = k\n",
    "\n",
    "    # ensure g_hat dir\n",
    "    if not os.path.exists(g_hat_path):\n",
    "        os.makedirs(g_hat_path)\n",
    "\n",
    "    # save positive triples\n",
    "    pd.DataFrame(pos_triples).to_csv('{}/positives_{}nn.tsv'.format(g_hat_path, k),\n",
    "                                     sep='\\t')\n",
    "    return prediction_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Ĝ (for different k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_info_list = []\n",
    "for k in range(knn_k_start, max_knn_k+1, knn_k_step):\n",
    "    prediction_info = pipeline(k, knn_products_generator(k, knn_indices, pos_train_and_valid))\n",
    "    prediction_info_list.append(prediction_info)\n",
    "    # save prediction info\n",
    "    pd.DataFrame(prediction_info_list).to_csv(g_hat_path + 'prediction_info.tsv', sep='\\t')\n",
    "    print(\"G_hat predicted for k={}\".format(k))\n",
    "    \n",
    "# generators = [\n",
    "#     knn_products_generator(knn_indices, pos_train_and_valid),\n",
    "#     knn_products_generator(knn_indices, pos_train_and_valid),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
